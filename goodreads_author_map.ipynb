{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "goodreads_author_map.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPWJgJhhCO8gWah2DoFSqBL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daniel-kelly/goodreads_author_map/blob/main/goodreads_author_map.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To run the code online, make sure to click the 'Open in Colab' button above. You will need to sign into a Google account to use it.\n",
        "\n",
        "### Installing external packages\n",
        "\n",
        "The key package we will use to plot data on the world map is geopandas, but there are some dependencies in mapclassify that will also be required.\n",
        "\n",
        "First we need to install these using pip"
      ],
      "metadata": {
        "id": "Ch2raPYoAF_a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install geopandas && pip install mapclassify"
      ],
      "metadata": {
        "id": "xX38k80fr0A8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing packages\n",
        "\n",
        "Now we import these above packages, as well as some pre-installed packages"
      ],
      "metadata": {
        "id": "YV2ul-lBA6C5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data management, visualisation and anaysis\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "\n",
        "# Progress bars\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Web scraping\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "# Map plotting\n",
        "import geopandas #, mapclassify"
      ],
      "metadata": {
        "id": "HhAiq20IBOOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Export your reading data from Goodreads\n",
        "To get your Goodreads data go to http://www.goodreads.com, log in and navigate to the 'My Books' tab. In the list of options on the left side of the page, go to **Tools/Import and export**. Click the **Export Library** button and once it has been generated it click and save it."
      ],
      "metadata": {
        "id": "xWGRuZJfDNJB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Upload Goodreads data\n",
        "Drag and drop your Goodreads export file (in .csv format) to the Colab workbook by dragging and dropping it to the folder on the left. \n",
        "\n",
        "If you are using my example export (goodreads_library_export_dk.csv), download it from the Github repository and drop it into the Google Colab Files tab on the left side of this page."
      ],
      "metadata": {
        "id": "gTyoMc7CW9Tz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_name=f'goodreads_library_export.csv'\n",
        "print(file_name)\n",
        "# Make sure the output of file_name matches your csv file name!"
      ],
      "metadata": {
        "id": "YYKlP7DH-8fZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the goodreads csv datafiles\n",
        "data=pd.read_csv(file_name) \n",
        "\n",
        "# Show the dataframe\n",
        "data"
      ],
      "metadata": {
        "id": "7YPP--tqsIAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Refining the dataframe\n",
        "We don't need a lot of the data columns so we will just take a list of authors from that have a book tagged 'read' and create a new dataframe"
      ],
      "metadata": {
        "id": "9RXxVeFgGS5_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We are only extracting the books with shelf label 'read'\n",
        "target_shelf='read'\n",
        "authors_read=data['Author'][data['Exclusive Shelf']==target_shelf]\n",
        "\n",
        "# If you want to filter by a personal shelf, uncomment the line below and specify the desired shelf in the variable target_shelf\n",
        "#authors_read=data['Author'][data['Bookshelves']==target_shelf] \n",
        "\n",
        "# New data frame just with 'read' authors\n",
        "ndf=pd.DataFrame(authors_read) \n",
        "\n",
        "ndf=ndf.reset_index()\n",
        "del ndf['index']"
      ],
      "metadata": {
        "id": "nXBd63BpC74W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gets a list of the 'read' authors\n",
        "author_list=list(ndf['Author'])\n",
        "\n",
        "# Count number of times each author appears in the list, i.e. number of books by each\n",
        "author_dict = {i:author_list.count(i) for i in author_list} "
      ],
      "metadata": {
        "id": "OTnO0qKHseGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adds a value for books read for each author in the dataframe\n",
        "for author in ndf['Author']:\n",
        "  ndf.loc[ndf['Author']==author,'Books read'] = author_dict[author]"
      ],
      "metadata": {
        "id": "LKlRjukbsl6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now that the number of books per author is in the dataframe we can delete the extra author entries\n",
        "ndfd=ndf.drop_duplicates()\n",
        "\n",
        "ndfd=ndfd.reset_index()\n",
        "del ndfd['index']\n",
        "\n",
        "# Now update the variable 'author list' (no duplicates)\n",
        "author_list=list(ndfd['Author']) "
      ],
      "metadata": {
        "id": "i_501cYBsm6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create columns for book titles per author and their corresponding goodreads ID numbers\n",
        "ndfd['Book titles'],ndfd['Book IDs']=ndfd['Author'],ndfd['Author'] "
      ],
      "metadata": {
        "id": "6anFOVWTsoKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the real book titles and ID numbers for each authors from the goodreads dataframe and add them to the new dataframe\n",
        "for i,j in enumerate(author_list):\n",
        "    ndfd['Book titles'].iloc[i] = list(data[data['Author']==j]['Title'])\n",
        "    ndfd['Book IDs'].iloc[i] = list(data[data['Author']==j]['Book Id'])\n",
        "\n",
        "# Create a column for the country each author comes from\n",
        "ndfd['Country']=['None']*len(ndfd['Author'])\n",
        "\n",
        "# Ignore the warnings"
      ],
      "metadata": {
        "id": "4C5YkzlespMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ndfd"
      ],
      "metadata": {
        "id": "ziZHIx_qX6hc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting author nationalities\n",
        "\n",
        "We will parse wikipedia pages for nationalities, then reference this data file to convert the nationality (adjectival) to country name. For example: *Stephen Edwin King (born September 21, 1947) is an American author of horror, supernatural fiction, suspense, crime, science-fiction, and fantasy novels.*\n",
        "\n",
        "The nationality will be recorded as **American**, which will be converted to **United States**. To get a list of adjectivals and countrys we can reference a [table](https://en.wikipedia.org/wiki/List_of_adjectival_and_demonymic_forms_for_countries_and_nations) on Wikipedia."
      ],
      "metadata": {
        "id": "ox97ANwopEIz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url='https://en.wikipedia.org/wiki/List_of_adjectival_and_demonymic_forms_for_countries_and_nations'\n",
        "page=requests.get(url)\n",
        "soup = BeautifulSoup(page.content,'html.parser')\n",
        "table = soup.find('table', {'class' : \"wikitable sortable\"})\n",
        "\n",
        "co,ad=[],[]\n",
        "rows = table.find_all('tr')\n",
        "for row in rows[1:]:\n",
        "  r=row.find_all('td')\n",
        "  for i in r[1].find_all('a'):\n",
        "      ad.append(i.text)\n",
        "      co.append(r[0].find_all('a')[0].text)\n",
        "adjs=pd.DataFrame(data=zip(co,ad),columns=['Country','Adjectival'])"
      ],
      "metadata": {
        "id": "NsqZlL18ZFn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will 'scrape' the wikipedia page for each author using the function below:"
      ],
      "metadata": {
        "id": "NClZxFT8eqlC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def wiki_paragraphs(\n",
        "    query\n",
        "            ):\n",
        "    '''\n",
        "    Function pulls a url of the form 'https://en.wikipedia.org/wiki/'+**query**\n",
        "    Where query is an input keyword\n",
        "    Requested url is scraped and all retrieved paragraphs (based on html <p> attribute) are returned in list form\n",
        "    '''\n",
        "    query.replace(' ','_')\n",
        "    url='https://en.wikipedia.org/wiki/'+query\n",
        "    page=requests.get(url)\n",
        "    soup = BeautifulSoup(page.content,'html.parser')\n",
        "\n",
        "    paragraphs=[]\n",
        "    for tag in soup.find_all():\n",
        "        if tag.name=='p':\n",
        "            paragraphs.append(tag.text)\n",
        "            \n",
        "    return paragraphs"
      ],
      "metadata": {
        "id": "7y3XPOewsv5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the function defined, we loop through every author in the list, try to locate their Wikipedia page and add their nationality to the dataframe. \n",
        "\n",
        "If no Wikipedia page is available for the author, or no mention of nationality is made in the first few paragraphs, their Goodreads profile will be scraped to see if a place of birth is listed. \n",
        "\n",
        "Bear in mind this can take a while if you have a lot of entries, though a progress bar should give a general idea of the timescale."
      ],
      "metadata": {
        "id": "ufJzoYVMfGbD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for author in tqdm(ndfd['Author'][:]):  # Loops through all authors  \n",
        "\n",
        "    paragraphs=wiki_paragraphs(author+' (writer)') \n",
        "    # First looks for wikipedia page with format 'Firstname_Lastname (writer)'\n",
        "    \n",
        "    if any('Other reasons this message' in pp for pp in paragraphs[:4]): # If no page found, tries again with form\n",
        "        paragraphs=wiki_paragraphs(author+' (author)')                   # 'Firstname_Lastname (author)'\n",
        "                \n",
        "    if any('Other reasons this message' in pp for pp in paragraphs[:4]): # If no page found, tries general format:\n",
        "        paragraphs=wiki_paragraphs(author+'')                            # 'Firstname_Lastname'\n",
        "    \n",
        "    # Flatten the paragraphs into a single list of words\n",
        "    pf=[]\n",
        "    for i in paragraphs[:5]:\n",
        "      for j in i.split(' '):\n",
        "        pf.append(j)\n",
        "\n",
        "    country_list=[]  # Create a blank list to store nationalities\n",
        "\n",
        "    # Loop through each word in the paragraph, check whether it appears in the list of adjectivals. \n",
        "    # If so, add the corresponding country to the author entry dataframe, then terminate the loop\n",
        "    for i in pf:\n",
        "      c=adjs['Country'].iloc[np.where(adjs['Adjectival']==i)[0]]\n",
        "      if len(c)>0:\n",
        "        country_list.append(adjs['Country'].iloc[np.where(adjs['Adjectival']==i)[0][0]])\n",
        "        break\n",
        "    \n",
        "    if len(country_list)!=0: \n",
        "        ndfd['Country'][ndfd['Author']==author]=country_list[0] \n",
        "\n",
        "\n",
        "    # If no countries were detected from the Wikipedia entries, check the author's Goodreads profile    \n",
        "    elif len(country_list)==0:\n",
        "\n",
        "        # First find a book by the author and use its Goodreads ID to navigate to the Goodreads page for said book\n",
        "        book_title=list(ndfd[ndfd['Author']==author]['Book titles'])[0][0].replace(' ','-').lower()\n",
        "        book_id=list(ndfd[ndfd['Author']==author]['Book IDs'])[0][0]\n",
        "\n",
        "        book_query=f'{book_id}-{book_title}'\n",
        "        book_url='https://www.goodreads.com/book/show/'+book_query\n",
        "\n",
        "        book_page=requests.get(book_url)\n",
        "        book_soup = BeautifulSoup(book_page.content,'html.parser')\n",
        "\n",
        "        # Find the URL to the author's page on the book page\n",
        "        # The URL can sometimes be found with different labels, so if statement checks both\n",
        "        author_url=book_soup.find(name='a',attrs={'class':'authorName'})\n",
        "        if author_url!=None:\n",
        "            author_url=author_url.attrs['href']\n",
        "            \n",
        "            author_page=requests.get(author_url)\n",
        "            author_soup=BeautifulSoup(author_page.content,'html.parser')\n",
        "\n",
        "            # Looks for the \"Born in\" field and takes the text entry\n",
        "            country_text=author_soup.find('div',text='Born')\n",
        "            if country_text!=None:\n",
        "              country_text=country_text.next_sibling.split('\\n')[1]\n",
        "        \n",
        "        elif author_url==None:\n",
        "            author_url=book_soup.find(attrs={'property':'og:url'})\n",
        "            \n",
        "            if author_url!=None:\n",
        "                author_url=author_url.attrs['content']\n",
        "                \n",
        "                author_page=requests.get(author_url)\n",
        "                author_soup=BeautifulSoup(author_page.content,'html.parser')\n",
        "                country_text=author_soup.find('div',text='Born')\n",
        "                if country_text!=None:\n",
        "                  country_text=country_text.next_sibling.split('\\n')[1]\n",
        "                \n",
        "        else:\n",
        "            continue\n",
        "        \n",
        "            # If the author was listed as being \"Born in\" a country on their Goodreads profile\n",
        "            # Add the listed country to the author entry in dataframe\n",
        "            for i in adjs['Country']:\n",
        "                if i in country_text:#\n",
        "                    country_list.append(i)\n",
        "                    ndfd['Country'][ndfd['Author']==author]=country_list[0] \n",
        "    \n",
        "    # If no country was found on either Wikipedia or Goodreads, the field is left blank     \n",
        "    elif len(country_list)==0:\n",
        "        ndfd['Country'][ndfd['Author']==author]=''\n",
        "\n",
        "    # Uncomment this line if you want to print out each author and their nationality as they are generated (full list will be available after regardless)\n",
        "    #print(f'{author}: {country_list}')"
      ],
      "metadata": {
        "id": "QPwzcQjqs3Nj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can check the data frame with the corresponding country for each author (we will save this as a csv/Excel-compatible file later on, don't worry)"
      ],
      "metadata": {
        "id": "IMwRUg1D14Yu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ndfd"
      ],
      "metadata": {
        "id": "06Xg1NXq13Uv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarly, we can see the authors for whom no country was detected. We do this by seeing which author's country entries are equal to 'None'"
      ],
      "metadata": {
        "id": "tRNBvTZh2Daa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ndfd[ndfd['Country']=='None']"
      ],
      "metadata": {
        "id": "0mf5q0Lis52d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plotting data on world map\n",
        "\n",
        "The final step is to visualise the distribution of books/authors across the map. We are going to use the geopandas package to accomplish this."
      ],
      "metadata": {
        "id": "HXTpGcIB0skG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = geopandas.datasets.get_path('naturalearth_lowres')\n",
        "\n",
        "# Create a new data set with the list of countries, their shapes (for the map) and extra data we can discard\n",
        "df = geopandas.read_file(path)"
      ],
      "metadata": {
        "id": "xq27YMD6tKiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create new columns to store the number of authors read and number of books read for each country\n",
        "df['Authors read']=df['pop_est']*0\n",
        "df['Books read']=df['pop_est']*0"
      ],
      "metadata": {
        "id": "7kPDFliGtMXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# As the map data only includes a polygon for the United Kingdom, we need to convert any authors listed as Scottish, Welsh etc. to UK\n",
        "britain=['United Kingdom of Great Britain and Northern Ireland','England','Scotland','Wales','Northern Ireland']\n",
        "for i in britain:\n",
        "  ndfd.loc[ndfd['Country']==i,'Country']='United Kingdom'\n",
        "\n",
        "# Likewise, we need to make sure the author country names match with the map data, hence we alter the author names for the USA\n",
        "ndfd.loc[ndfd['Country']=='United States','Country']='United States of America'"
      ],
      "metadata": {
        "id": "wy1CTZNV1pC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We get a list of all the countries associated with the Goodreads author list\n",
        "country_list=np.unique(ndfd['Country'].astype('string'))\n",
        "\n",
        "# We loop through each and add the number of books and authors for each country\n",
        "for i in country_list:\n",
        "    author_counter=(ndfd['Country']==i).sum()\n",
        "    book_counter=((ndfd['Country']==i)*(ndfd['Books read'])).sum()\n",
        "    df.loc[df['name']==i,'Authors read']=author_counter\n",
        "    df.loc[df['name']==i,'Books read']=book_counter"
      ],
      "metadata": {
        "id": "wy5Cq9WXtMsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For countries that do not appear in the author list, we assign them a value of 'NaN' (not a number)\n",
        "df.loc[df['Books read']==0,'Books read']=np.nan\n",
        "df.loc[df['Authors read']==0,'Authors read']=np.nan\n",
        "\n",
        "# And delete the irrelevant metrics\n",
        "del df['pop_est'],df['continent'],df['iso_a3'],df['gdp_md_est']"
      ],
      "metadata": {
        "id": "ofIABAPZtPPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can now see the values for books read and authors read for each country\n",
        "df[df['Books read']>0]"
      ],
      "metadata": {
        "id": "aPzV-mbl4Bql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving data\n",
        "We can save the data files to csv formats (can be opened in Excel) for reference. The `country_stats` file gives a list of all the countries in the author list, with the number of authors and books read from each.\n",
        "\n",
        "The `author_countries` gives a list of all the authors from the Goodreads export, with the corresponding list of books read, books IDs, and country."
      ],
      "metadata": {
        "id": "XCBTRZkv4T16"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(f'country_stats.csv')\n",
        "ndfd.to_csv(f'author_countries.csv')"
      ],
      "metadata": {
        "id": "EdkNVKjntQNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The maps\n",
        "\n",
        "Now we can plot the maps, the plotting is fairly complex but is mostly just formatting. Values that can be changed to customise each plot are marked by in-line comments."
      ],
      "metadata": {
        "id": "Pm2EndEg41QM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose the minimum and maximum values to display\n",
        "# The default is the respective minimum (usually 1) and maximum number of books read from the country dataframe above\n",
        "vmin=np.nanmin(df['Books read'])\n",
        "vmax=np.nanmax(df['Books read'])\n",
        "\n",
        "\n",
        "# We create a colourmap (don't mess with this!)\n",
        "cmap = plt.cm.jet\n",
        "cmaplist = [cmap(i) for i in range(cmap.N)]\n",
        "cmap = matplotlib.colors.LinearSegmentedColormap.from_list('Custom cmap', cmaplist, cmap.N)\n",
        "\n",
        "# Create the figure\n",
        "fig,ax=plt.subplots(figsize=(20.0,8.0),)\n",
        "df.plot(column='Books read',legend=True,ax=ax,cmap=cmap,missing_kwds={'color': 'lightgrey'},vmin=vmin,vmax=vmax)\n",
        "\n",
        "# Comment out this line if you do not want the outlines of the countries plotted\n",
        "df['geometry'].boundary.plot(ax=ax,color='black',linewidth=0.2)\n",
        "\n",
        "ax.set_title('Books read per country')\n",
        "ax.set_xticks([])\n",
        "ax.set_yticks([])\n",
        "ax.set_xlim(-180,180)\n",
        "ax.set_ylim(-90,90)\n",
        "\n",
        "\n",
        "\n",
        "# Adding some text to the bottom of the plot\n",
        "ax.set_xlabel('In total: {} authors read, from {} different countries \\n Nationality not found for {} authors: {}'.\n",
        "              format(int(df['Authors read'].sum()),len(np.unique(ndfd['Country'])),(ndfd['Country']=='None').sum(),np.unique(ndfd['Author'][ndfd['Country']=='None'])),fontsize=8,)\n",
        "\n",
        "# Saving the figure in a PNG format (dpi is image quality)\n",
        "plt.savefig(f'books_read.png',bbox_inches='tight',padding=0,dpi=200)"
      ],
      "metadata": {
        "id": "7F7mTn8-BCbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rather than continuous values, we can also plot ranges of the values:"
      ],
      "metadata": {
        "id": "ch8PAhLDCoxz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # Choose the minimum and maximum values to display\n",
        "# The default is the respective minimum (usually 1) and maximum number of books read from the country dataframe above\n",
        "vmin=np.nanmin(df['Books read'])\n",
        "vmax=np.nanmax(df['Books read'])\n",
        "\n",
        "# We create a colourmap (don't mess with this!)\n",
        "cmap = plt.cm.jet\n",
        "cmaplist = [cmap(i) for i in range(cmap.N)]\n",
        "cmap = matplotlib.colors.LinearSegmentedColormap.from_list('Custom cmap', cmaplist, cmap.N)\n",
        "\n",
        "# Here we can define the ranges of the 'bins' used for plotting, the interval is the width of the bin\n",
        "interval=10\n",
        "bounds=[1]\n",
        "[bounds.append(int(i)) for i in np.arange(interval,vmax+interval,interval)]\n",
        "norm = matplotlib.colors.BoundaryNorm(bounds, cmap.N)\n",
        "\n",
        "# Create the figure\n",
        "fig,ax=plt.subplots(figsize=(20.0,8.0),)\n",
        "df.plot(column='Books read',legend=False,scheme='user_defined',ax=ax,cmap=cmap,classification_kwds={'bins':bounds[1:]},missing_kwds={'color': 'lightgrey'},)\n",
        "\n",
        "# Comment out this line if you do not want the outlines of the countries plotted\n",
        "df['geometry'].boundary.plot(ax=ax,color='black',linewidth=0.2)\n",
        "\n",
        "ax.set_title('Books read per country')\n",
        "ax.set_xticks([])\n",
        "ax.set_yticks([])\n",
        "ax.set_xlim(-180,180)\n",
        "ax.set_ylim(-90,90)\n",
        "\n",
        "# Creating a colourbar for ranges\n",
        "cax = fig.add_axes([.85, 0.11, 0.01, 0.77])\n",
        "sm = plt.cm.ScalarMappable(cmap=cmap, norm=plt.Normalize(vmin=bounds[1], vmax=bounds[-1]))\n",
        "sm._A = []\n",
        "cb = fig.colorbar(sm, cax=cax,cmap=cmap, norm=norm, ticks=bounds, boundaries=bounds,)\n",
        "\n",
        "# Adding some text to the bottom of the plot\n",
        "ax.set_xlabel('In total: {} authors read, from {} different countries \\n Nationality not found for {} authors: {}'.\n",
        "              format(int(df['Authors read'].sum()),len(np.unique(ndfd['Country'])),(ndfd['Country']=='None').sum(),np.unique(ndfd['Author'][ndfd['Country']=='None'])),fontsize=8,)\n",
        "\n",
        "# Saving the figure in a PNG format\n",
        "plt.savefig(f'books_read_range.png',bbox_inches='tight',padding=0,dpi=200)"
      ],
      "metadata": {
        "id": "hNc-k8aZtSiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It may be the case that you do not want the maximum value to be the upper limit of the bins. In my case, the number of books from the US and UK far outweigh all others, so countries with less 10 books cannot be distinguished.\n",
        "\n",
        "In this case, we can set the upper limit to 10+ (saturating the US and UK), so the values between 0-10 can be distinguished more clearly."
      ],
      "metadata": {
        "id": "cTwFz9r86DLy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we manually fix the minimum and maximum values of the ranges\n",
        "vmin=0\n",
        "vmax=10\n",
        "\n",
        "cmap = plt.cm.jet\n",
        "cmaplist = [cmap(i) for i in range(cmap.N)]\n",
        "cmap = matplotlib.colors.LinearSegmentedColormap.from_list('Custom cmap', cmaplist, cmap.N)\n",
        "\n",
        "# We can now set a smaller interval\n",
        "interval=2\n",
        "bounds = np.arange(vmin,vmax+interval,interval)\n",
        "norm = matplotlib.colors.BoundaryNorm(bounds, cmap.N)\n",
        "\n",
        "fig,ax=plt.subplots(figsize=(20.0,8.0),)\n",
        "df.plot(column='Books read',legend=False,scheme='user_defined',ax=ax,cmap=cmap,classification_kwds={'bins':bounds[1:]},missing_kwds={'color': 'lightgrey'})\n",
        "df['geometry'].boundary.plot(ax=ax,color='k',linewidth=0.2)\n",
        "\n",
        "ax.set_title('Books read per country')\n",
        "ax.set_xticks([])\n",
        "ax.set_yticks([])\n",
        "ax.set_xlim(-180,180)\n",
        "ax.set_ylim(-90,90)\n",
        "\n",
        "cax = fig.add_axes([.85, 0.11, 0.01, 0.77])\n",
        "sm = plt.cm.ScalarMappable(cmap=cmap, norm=plt.Normalize(vmin=bounds[1], vmax=bounds[-1]-interval))\n",
        "sm._A = []\n",
        "cb = fig.colorbar(sm, cax=cax,cmap=cmap, norm=norm, ticks=bounds, boundaries=bounds,)\n",
        "cax.set_yticklabels(np.hstack([bounds[:-1].astype(int),f'{bounds[-1].astype(int)}+']))\n",
        "\n",
        "ax.set_xlabel('In total: {} authors read, from {} different countries \\n Nationality not found for {} authors: {}'.\n",
        "              format(int(df['Authors read'].sum()),len(np.unique(ndfd['Country'])),(ndfd['Country']=='None').sum(),np.unique(ndfd['Author'][ndfd['Country']=='None'])),fontsize=8,)\n",
        "\n",
        "plt.savefig('books_read_range_limited.png',bbox_inches='tight',padding=0,dpi=200)"
      ],
      "metadata": {
        "id": "I_jSSzXXtc2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can do the exact same thing for the number of authors read for each country. We can also change the colourmap (find a full list of options [here](https://matplotlib.org/stable/tutorials/colors/colormaps.html))"
      ],
      "metadata": {
        "id": "BkShjham7hIQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You can see here we need the minimum and maximum of different column: 'Authors read'\n",
        "vmin=np.nanmin(df['Authors read'])\n",
        "vmax=np.nanmax(df['Authors read'])\n",
        "\n",
        "# Now we can change the colourmap\n",
        "cmap = plt.cm.viridis\n",
        "cmaplist = [cmap(i) for i in range(cmap.N)]\n",
        "cmap = matplotlib.colors.LinearSegmentedColormap.from_list('Custom cmap', cmaplist, cmap.N)\n",
        "\n",
        "interval=5\n",
        "bounds=[1]\n",
        "[bounds.append(int(i)) for i in np.arange(interval,vmax+interval,interval)]\n",
        "norm = matplotlib.colors.BoundaryNorm(bounds, cmap.N)\n",
        "\n",
        "fig,ax=plt.subplots(figsize=(20.0,8.0),)\n",
        "\n",
        "# And we plot the 'Authors read' column\n",
        "df.plot(column='Authors read',legend=False,scheme='user_defined',ax=ax,cmap=cmap,classification_kwds={'bins':bounds[1:]},missing_kwds={'color': 'lightgrey'},)\n",
        "\n",
        "# Comment out this line if you do not want the outlines of the countries plotted\n",
        "df['geometry'].boundary.plot(ax=ax,color='black',linewidth=0.2)\n",
        "\n",
        "# We change the title of the plot\n",
        "ax.set_title('Authors read per country')\n",
        "ax.set_xticks([])\n",
        "ax.set_yticks([])\n",
        "ax.set_xlim(-180,180)\n",
        "ax.set_ylim(-90,90)\n",
        "\n",
        "cax = fig.add_axes([.85, 0.11, 0.01, 0.77])\n",
        "sm = plt.cm.ScalarMappable(cmap=cmap, norm=plt.Normalize(vmin=bounds[1], vmax=bounds[-1]))\n",
        "sm._A = []\n",
        "cb = fig.colorbar(sm, cax=cax,cmap=cmap, norm=norm, ticks=bounds, boundaries=bounds,)\n",
        "\n",
        "ax.set_xlabel('In total: {} authors read, from {} different countries \\n Nationality not found for {} authors: {}'.\n",
        "              format(int(df['Authors read'].sum()),len(np.unique(ndfd['Country'])),(ndfd['Country']=='None').sum(),np.unique(ndfd['Author'][ndfd['Country']=='None'])),fontsize=8,)\n",
        "\n",
        "# And of course we change the name of the saved file\n",
        "plt.savefig(f'authors_read_range.png',bbox_inches='tight',padding=0,dpi=200)"
      ],
      "metadata": {
        "id": "D26Rcdd7AXNH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}